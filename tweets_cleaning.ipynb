{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweets_cleaning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "grwNs0Oeil_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('som1.csv', encoding = 'unicode_escape')\n",
        "#full_text is fine\n",
        "\n",
        "df_copy = df\n",
        "print(len(df_copy))\n",
        "serlis=df_copy.duplicated().tolist()\n",
        "print(serlis.count(True)) #7479\n",
        "\n",
        "serlis=df_copy.duplicated(['full_text']).tolist()\n",
        "print(serlis.count(True))\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.tag import pos_tag,map_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "pstem = PorterStemmer()\n",
        "lem = WordNetLemmatizer()\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def clean_df(df_copy):\n",
        "    \n",
        "    #DROPS\n",
        "    #CHOOSE EITHER TO DROP ALL-ROW DUPLICATES OR FULL_TEXT DUPLICATES\n",
        "    df_copy=df_copy.drop_duplicates(['full_text']) #3377 left after this\n",
        "    df_copy=df_copy.reset_index(drop=True)\n",
        "    df_copy=df_copy.drop(['place','coordinates','geo','id_str'],axis=1)\n",
        "    #df_copy=\n",
        "    \n",
        "    # BASIC CLEANING FUNCTION\n",
        "    for i in range(len(df_copy)):\n",
        "        txt = df_copy.loc[i][\"full_text\"]\n",
        "        txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#username-tags\n",
        "        txt=re.sub(r'^[RT]+','',txt)#RT-tags\n",
        "        txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#URLs\n",
        "        txt=re.sub(\"[^a-zA-Z]\", \" \",txt)#hashtags\n",
        "        df_copy.at[i,\"full_text\"]=txt\n",
        "    #POS-TAGGING AND SENTIMENT SCORE\n",
        "    li_swn=[]\n",
        "    li_swn_pos=[]\n",
        "    li_swn_neg=[]\n",
        "    missing_words=[]\n",
        "    for i in range(len(df_copy.index)):\n",
        "        text = df_copy.loc[i]['full_text']\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tagged_sent = pos_tag(tokens)\n",
        "        store_it = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]\n",
        "        #print(\"Tagged Parts of Speech:\",store_it)\n",
        "\n",
        "        pos_total=0\n",
        "        neg_total=0\n",
        "        for word,tag in store_it:\n",
        "            if(tag=='NOUN'):\n",
        "                tag='n'\n",
        "            elif(tag=='VERB'):\n",
        "                tag='v'\n",
        "            elif(tag=='ADJ'):\n",
        "                tag='a'\n",
        "            elif(tag=='ADV'):\n",
        "                tag = 'r'\n",
        "            else:\n",
        "                tag='nothing'\n",
        "\n",
        "            if(tag!='nothing'):   \n",
        "                concat = word+'.'+tag+'.01'\n",
        "                try:\n",
        "                    this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                    this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                    #print(word,tag,':',this_word_pos,this_word_neg)\n",
        "                except Exception as e:\n",
        "                    wor = lem.lemmatize(word)\n",
        "                    concat = wor+'.'+tag+'.01'\n",
        "                    # Checking if there's a possiblity of lemmatized word be accepted into SWN corpus\n",
        "                    try:\n",
        "                        this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                        this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                    except Exception as e:\n",
        "                        wor = pstem.stem(word)\n",
        "                        concat = wor+'.'+tag+'.01'\n",
        "                        # Checking if there's a possiblity of lemmatized word be accepted\n",
        "                        try:\n",
        "                            this_word_pos=swn.senti_synset(concat).pos_score()\n",
        "                            this_word_neg=swn.senti_synset(concat).neg_score()\n",
        "                        except:\n",
        "                            missing_words.append(word) \n",
        "                            continue\n",
        "                pos_total+=this_word_pos\n",
        "                neg_total+=this_word_neg\n",
        "        li_swn_pos.append(pos_total)\n",
        "        li_swn_neg.append(neg_total)\n",
        "\n",
        "        if(pos_total!=0 or neg_total!=0):\n",
        "            if(pos_total>neg_total):\n",
        "                li_swn.append(1)\n",
        "            else:\n",
        "                li_swn.append(-1)\n",
        "        else:\n",
        "            li_swn.append(0)\n",
        "    # end-of pos-tagging&sentiment\n",
        "    \n",
        "    #3-rd for loop    \n",
        "    #LEMMATIZING,STEMMING,STOP-WORDS\n",
        "    for i in range(len(df_copy.index)):\n",
        "        text = df_copy.loc[i]['full_text']\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        for j in range(len(tokens)):\n",
        "            tokens[j] = lem.lemmatize(tokens[j])\n",
        "            tokens[j] = pstem.stem(tokens[j])\n",
        "\n",
        "        tokens_sent=' '.join(tokens)\n",
        "        df_copy.at[i,\"full_text\"] = tokens_sent\n",
        "        \n",
        "    df_copy.insert(5,\"pos_score\",li_swn_pos,True)\n",
        "    df_copy.insert(6,\"neg_score\",li_swn_neg,True)\n",
        "    df_copy.insert(7,\"sent_score\",li_swn,True)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return df_copy\n",
        "\n",
        "st=time.time()\n",
        "df_copy = clean_df(df_copy)\n",
        "end=time.time()\n",
        "\n",
        "# df_copy.loc[4]\n",
        "print(end-st)#\n",
        "# df_copy\n",
        "# takes(no senti) 0.7440352439880371 with txt less..so go with it\n",
        "# takes(no senti) 3.600219488143921 with df_copy.loc\n",
        "print(df_copy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}