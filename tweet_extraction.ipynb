{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_extraction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKSAzUMdiD4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tweepy\n",
        "import jsonpickle\n",
        "import csv\n",
        "import time\n",
        "st = time.time()\n",
        "API_KEY=\"**************************\"\n",
        "API_SECRET=\"***********************************\"\n",
        "\n",
        "auth = tweepy.AppAuthHandler(API_KEY,API_SECRET)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
        "#wait_on_rate_limit – Whether or not to automatically wait for rate limits to replenish\n",
        "#wait_on_rate_limit_notify – Whether or not to print a notification when Tweepy is waiting for rate limits to replenish\n",
        "# the sleep mode is automatically enabled with above 2 args\n",
        "\n",
        "\n",
        "tweetsPerQuery = 100#this is the maximum provided by API\n",
        "max_tweets = 100000000 # just for the sake of While loop\n",
        "fName = 'sb_04_08_19.txt' # where i save the tweets\n",
        "\n",
        "# No sinceId and max_id ..Get whathever you have exhaustively\n",
        "since_id = None\n",
        "max_id = -1\n",
        "tweet_count = 0\n",
        "print(\"Downloading the tweeets..takes some time..\")\n",
        "\n",
        "search_query=\"#covid19\"\n",
        "x=0\n",
        "with open(fName,'w') as f:\n",
        "    print(\"Downloading hashtag\" + search_query)\n",
        "    while(tweet_count<max_tweets):\n",
        "        try:\n",
        "            if(max_id<=0):\n",
        "                if(not since_id):\n",
        "                    new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"en\",tweet_mode='extended')\n",
        "\n",
        "                else:\n",
        "                    new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"en\",tweet_mode='extended',since_id=since_id)\n",
        "            else:\n",
        "                if(not since_id):\n",
        "                    new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"en\",tweet_mode='extended',max_id=str(max_id-1))\n",
        "                else:\n",
        "                    new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"en\",tweet_mode='extended',max_id=str(max_id-1),since_id=since_id)\n",
        "\n",
        "            # Tweets Exhausted\n",
        "            if(not new_tweets):\n",
        "                print(\"No more tweets found!!\")\n",
        "                break\n",
        "            # write all the new_tweets to a json file\n",
        "            for tweet in new_tweets:\n",
        "                f.write(jsonpickle.encode(tweet._json,unpicklable=False)+'\\n')\n",
        "                tweet_count+=len(new_tweets)\n",
        "                print(\"Successfully downloaded {0} tweets\".format(tweet_count))\n",
        "                max_id=new_tweets[-1].id\n",
        "        # in case of any error\n",
        "        except tweepy.TweepError as e:\n",
        "                print(\"Some error!!:\"+str(e))\n",
        "                break\n",
        "end = time.time()\n",
        "\n",
        "print(\"A total of {0} tweets are downloaded and saved to {1}\".format(tweet_count,fName))\n",
        "print(\"Total time taken is \",end-st,\"seconds.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}